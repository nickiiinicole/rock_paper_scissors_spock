# Pr谩ctica: Agentes Inteligentes - Piedra, Papel, Tijeras (RPS)

![Python Version](https://img.shields.io/badge/python-3.x-blue?style=flat-square&logo=python)
![Status](https://img.shields.io/badge/status-development-orange?style=flat-square)
![License](https://img.shields.io/badge/license-MIT-green?style=flat-square)

Se propone programar un agente inteligente como soluci贸n al entorno de tareas del juego **Piedra, Papel, Tijeras**, siguiendo las directrices de modelado propuestas en el cap铆tulo 2 _Intelligent Agents_ del libro _IA: A modern approach, Russell & Norvig_.

##  ndice

1. [Especificaci贸n del entorno de tareas](#1-especificaci贸n-del-entorno-de-tareas)
2. [Identificaci贸n del tipo de agente y estructura](#2-identificaci贸n-del-tipo-de-agente-y-estructura)
3. [Implementaci贸n en Python](#3-implementaci贸n-en-python)
4. [Extensi贸n a RPS + Lagarto Spock](#4-extensi贸n-a-rps--lagarto-spock)
5. [Bibliograf铆a](#bibliograf铆a)

---

## 1. Especificaci贸n del entorno de tareas

Siguiendo el ep铆grafe _"2.3.2 Properties of task environments"_ de Russell & Norvig, se especifican las caracter铆sticas del entorno del RPS.

### Tabla de caracter铆sticas

| Entorno de tareas | Observable | Agentes | Determinista | Epis贸dico | Est谩tico | Discreto | Conocido |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **RPS** | PARTIALLY | MULTI & COMPETITIVE | STOCHASTIC | SEQUENTIAL | STATIC | DISCRETE | KNOW |


### Justificaci贸n

> **Nota:** A continuaci贸n se justifica cada una de las propiedades seleccionadas en la tabla:

* **Observable Parcialmente :**  no vemos la mano del oponente hasta que saca
* **Multi Agente:** hay dos jugadores compitiendo
* **Estoc谩stico:** , ya que el ganar de una probabilidad de resultado 
* **Epis贸dico:** solo importa maximizar la recompensa inmediata, no afecta a futuro
* **Est谩tico:** no cambia el entorno
* **Discreto:** es una iteracci贸n fija, solo sacas piedra, papel, tijera 
* **Conocido:** conoce las reglas del juego


---

## 2. Identificaci贸n del tipo de agente y estructura

Para resolver este problema, se ha seleccionado un **[INDICAR TIPO DE AGENTE: Ej. Agente Reactivo Basado en Modelos]**.

### Modelo del Agente

A continuaci贸n se muestra el diagrama del modelo elegido, adaptado espec铆ficamente al contexto del juego RPS:

![Modelo Agente](./doc/modelo_AI.png)
*(Recuerda sustituir esta imagen por tu propio diagrama donde se vean los componentes espec铆ficos de tu agente).*

### Componentes y Justificaci贸n

El agente se estructura con los siguientes componentes mostrados en la figura:

1.  **Sensores:** [Descripci贸n de qu茅 percibe el agente...]
2.  **Estado Interno:** [Descripci贸n de qu茅 memoria guarda el agente...]
3.  **Reglas de condici贸n-acci贸n:** [Descripci贸n de la l贸gica de decisi贸n...]
4.  **Actuadores:** [Descripci贸n de c贸mo ejecuta la acci贸n el agente...]

---

## 3. Implementaci贸n en Python

La implementaci贸n se ha realizado en Python siguiendo los principios **SOLID**, haciendo especial 茅nfasis en:
* **SRP (Single Responsibility Principle):** Modularizaci贸n del c贸digo para que cada funci贸n tenga una 煤nica responsabilidad.
* **OCP (Open/Closed Principle):** Dise帽o preparado para a帽adir nuevas armas (como Lagarto y Spock) sin modificar el c贸digo fuente original.

### Estrategia del Agente

La l贸gica principal de decisi贸n reside en la funci贸n `get_computer_action()`. La estrategia implementada para maximizar el **rendimiento** consiste en:

> [Describe aqu铆 tu estrategia creativa. Ej: "El agente utiliza una cadena de Markov para predecir el siguiente movimiento del usuario bas谩ndose en su historial reciente..."]

### Diagrama de flujo del programa

![Table Driven Agent Program](./doc/table_driven_agent_program.png)

### Ejemplo de C贸digo

El n煤cleo de la decisi贸n se encuentra en el siguiente bloque:

```python
def get_computer_action(user_action):
    # Ejemplo de l贸gica simplificada
    if user_action == "Rock":
        return "Paper"
    # ... l贸gica real ...
    return action