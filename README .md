# Pr谩ctica: Agentes Inteligentes - Piedra, Papel, Tijeras (RPS)

![Python Version](https://img.shields.io/badge/python-3.x-blue?style=flat-square&logo=python)
![Status](https://img.shields.io/badge/status-development-orange?style=flat-square)
![License](https://img.shields.io/badge/license-MIT-green?style=flat-square)

Se propone programar un agente inteligente como soluci贸n al entorno de tareas del juego **Piedra, Papel, Tijeras**, siguiendo las directrices de modelado propuestas en el cap铆tulo 2 _Intelligent Agents_ del libro _IA: A modern approach, Russell & Norvig_.

##  ndice

1. [Especificaci贸n del entorno de tareas](#1-especificaci贸n-del-entorno-de-tareas)
2. [Identificaci贸n del tipo de agente y estructura](#2-identificaci贸n-del-tipo-de-agente-y-estructura)
3. [Implementaci贸n en Python](#3-implementaci贸n-en-python)
4. [Extensi贸n a RPS + Lagarto Spock (PENDIENTE)](#4-extensi贸n-a-rps--lagarto-spock)


---

## 1. Especificaci贸n del entorno de tareas

Siguiendo el ep铆grafe _"2.3.2 Properties of task environments"_ de Russell & Norvig, se especifican las caracter铆sticas del entorno del RPS.

### Tabla de caracter铆sticas

| Entorno de tareas | Observable | Agentes | Determinista | Epis贸dico | Est谩tico | Discreto | Conocido |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **RPS** | PARTIALLY | MULTI & COMPETITIVE | STOCHASTIC | SEQUENTIAL | STATIC | DISCRETE | KNOW |


### Justificaci贸n

> **Nota:** A continuaci贸n se justifica cada una de las propiedades seleccionadas en la tabla:

* **Observable Parcialmente :**  no vemos la mano del oponente hasta que saca
* **Multi Agente:** hay dos jugadores compitiendo
* **Estoc谩stico:** , ya que el ganar de una probabilidad de resultado 
* **Secuencial:** El agente consulta el historial (memoria) de las jugadas anteriores para decidir, por lo que los episodios est谩n conectados.
* **Est谩tico:** no cambia el entorno
* **Discreto:** es una iteracci贸n fija, solo sacas piedra, papel, tijera 
* **Conocido:** conoce las reglas del juego


---

## 2. Identificaci贸n del tipo de agente y estructura

Se ha seleccionado un **Agente Reactivo Basado en Modelos**.

### Modelo del Agente

![Diagrama del Agente - RPS](./img/image_agent.png)

### Componentes y Justificaci贸n

El agente necesita mantener un registro del pasado para predecir el futuro. Sus componentes son:

1.  **Sensores:** Reciben la jugada del oponente del turno anterior (input del usuario).
2.  **Estado Interno (Memoria):** Una estructura de datos (lista o diccionario) que almacena el historial de todas las jugadas del oponente hasta el momento. Sin esto, el agente ser铆a ciego a los patrones de comportamiento.
3.  **Reglas de condici贸n-acci贸n (Estrategia):** Basado en el historial, el oponente saca 'Piedra' el 60% de las veces".
    * *Regla de Decisi贸n:* "Si lo m谩s probable es 'Piedra', mi acci贸n es 'Papel'".
4.  **Actuadores:** La funci贸n que devuelve la jugada elegida (`return "Paper"`) y la muestra en la consola.



## 3. Implementaci贸n en Python

La implementaci贸n se ha realizado en Python siguiendo los principios **SOLID**, haciendo especial 茅nfasis en:
* **SRP (Single Responsibility Principle):** Modularizaci贸n del c贸digo para que cada funci贸n tenga una 煤nica responsabilidad.
* **OCP (Open/Closed Principle):** Dise帽o preparado para a帽adir nuevas armas (como Lagarto y Spock) sin modificar el c贸digo fuente original. 

### Estrategia del Agente

La l贸gica principal reside en `get_computer_action()`. Para maximizar el rendimiento, se ha implementado una estrategia de **An谩lisis de Frecuencia Hist贸rica**:

> El agente utiliza un diccionario para contar cu谩ntas veces ha sacado el usuario Piedra, Papel o Tijeras. Calcula cu谩l es la jugada m谩s frecuente del rival (Moda) y selecciona autom谩ticamente la acci贸n que vence a esa tendencia. Si no hay datos suficientes, act煤a aleatoriamente.

### Ejemplo de C贸digo

El n煤cleo de la decisi贸n implementa esta l贸gica de conteo y contraataque:

```python
def get_computer_action(user_history):
    """
    Determina la acci贸n bas谩ndose en el historial del oponente.
    Estrategia: Counter-Move sobre la jugada m谩s frecuente (Moda).
    """
    import random
    
    game_rules = {
        "Rock": "Paper",
        "Paper": "Scissors",
        "Scissors": "Rock"
    }
    
    # 1. Si no hay datos, jugar aleatorio
    if not user_history:
        return random.choice(list(game_rules.keys()))
    
    # 2. Calcular la jugada m谩s frecuente del usuario (Modelo)
    most_frequent_move = max(set(user_history), key=user_history.count)
    
    # 3. Elegir la acci贸n que gana a esa jugada (Regla de decisi贸n)
    prediction = game_rules[most_frequent_move]
    
    return prediction

