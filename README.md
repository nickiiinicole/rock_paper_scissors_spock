# Pr谩ctica: Agentes Inteligentes - Piedra, Papel, Tijeras (RPS)

![Python Version](https://img.shields.io/badge/python-3.x-blue?style=flat-square&logo=python)
![Status](https://img.shields.io/badge/status-development-orange?style=flat-square)
![License](https://img.shields.io/badge/license-MIT-green?style=flat-square)

Se propone programar un agente inteligente como soluci贸n al entorno de tareas del juego **Piedra, Papel, Tijeras**, siguiendo las directrices de modelado propuestas en el cap铆tulo 2 _Intelligent Agents_ del libro _IA: A modern approach, Russell & Norvig_.

##  ndice

1. [Especificaci贸n del entorno de tareas](#1-especificaci贸n-del-entorno-de-tareas)
2. [Identificaci贸n del tipo de agente y estructura](#2-identificaci贸n-del-tipo-de-agente-y-estructura)
3. [Implementaci贸n en Python](#3-implementaci贸n-en-python)
4. [Extensi贸n a RPS + Lagarto Spock](#4-extensi贸n-a-rps--lagarto-spock)


---

## 1. Especificaci贸n del entorno de tareas

Siguiendo el ep铆grafe _"2.3.2 Properties of task environments"_ de Russell & Norvig, se especifican las caracter铆sticas del entorno del RPS.

### Tabla de caracter铆sticas

| Entorno de tareas | Observable | Agentes | Determinista | Epis贸dico | Est谩tico | Discreto | Conocido |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **RPS** | PARTIALLY | MULTI & COMPETITIVE | STOCHASTIC | SEQUENTIAL | STATIC | DISCRETE | KNOW |


### Justificaci贸n

> **Nota:** A continuaci贸n se justifica cada una de las propiedades seleccionadas en la tabla:

* **Observable Parcialmente :**  no vemos la mano del oponente hasta que saca
* **Multi Agente:** hay dos jugadores compitiendo
* **Estoc谩stico:** , ya que el ganar de una probabilidad de resultado 
* **Secuencial:** El agente consulta el historial (memoria) de las jugadas anteriores para decidir, por lo que los episodios est谩n conectados.
* **Est谩tico:** no cambia el entorno
* **Discreto:** es una iteracci贸n fija, solo sacas piedra, papel, tijera 
* **Conocido:** conoce las reglas del juego


---

## 2. Identificaci贸n del tipo de agente y estructura

Se ha seleccionado un **Agente Reactivo Basado en Modelos**.

### Modelo del Agente

![Diagrama del Agente - RPS](./img/image_agent.png)

### Componentes y Justificaci贸n

El agente necesita mantener un registro del pasado para predecir el futuro. Sus componentes son:

1.  **Sensores:** Reciben la jugada del oponente del turno anterior (input del usuario).
2.  **Estado Interno (Memoria):** Una estructura de datos (lista o diccionario) que almacena el historial de todas las jugadas del oponente hasta el momento. Sin esto, el agente ser铆a ciego a los patrones de comportamiento.
3.  **Reglas de condici贸n-acci贸n (Estrategia):** Basado en el historial, el oponente saca 'Piedra' el 60% de las veces".
    * *Regla de Decisi贸n:* "Si lo m谩s probable es 'Piedra', mi acci贸n es 'Papel'".
4.  **Actuadores:** La funci贸n que devuelve la jugada elegida (`return "Paper"`) y la muestra en la consola.


## 3. Implementaci贸n en Python

La implementaci贸n se ha realizado siguiendo los principios **SOLID**: 
* **SRP (Single Responsibility Principle):** Se ha modularizado el c贸digo separando responsabilidades:
    * `Victories` (Diccionario): Define las reglas de datos y relaciones de victoria.
    * `IntelligentAgent`: Clase encargada 煤nicamente de gestionar la memoria y decidir el siguiente movimiento.
    * `assess_game`: Funci贸n encargada 煤nicamente de gestionar la visualizaci贸n del resultado (UI), delegando la l贸gica de decisi贸n.
* **OCP (Open/Closed Principle):** El dise帽o est谩 "cerrado a modificaci贸n" pero "abierto a extensi贸n". Al sustituir las cadenas de `if/elif` por un diccionario de reglas (`Victories`), fue posible a帽adir nuevas armas sin modificar la l贸gica interna de las funciones de evaluaci贸n.

### Estrategia del Agente (IntelligentAgent)

La l贸gica de decisi贸n se invoca desde `get_computer_action()`, pero reside en la clase `IntelligentAgent`. Se ha implementado un **Agente Reactivo Basado en Modelos** con una estrategia de **An谩lisis de Frecuencia Hist贸rica:D**:

> El agente mantiene una memoria interna (`self.history`) de todas las jugadas del usuario. En cada turno, utiliza la herramienta `Counter` para calcular la **moda** (la jugada que m谩s repite el rival) y consulta el diccionario de reglas para seleccionar autom谩ticamente la acci贸n espec铆fica que la derrota

### Ejemplo de C贸digo

`IntelligentAgent` implementa la l贸gica de predicci贸n y contraataque:

```python
class IntelligentAgent:
    def __init__(self):
        self.history = []

    def get_move(self):
        # 1. Si no hay datos (primera ronda), jugar aleatorio
        if not self.history:
             selection = random.randint(0, len(GameAction) - 1)
             return GameAction(selection)
        
        # 2. INTELIGENCIA: Calcular la jugada m谩s frecuente del usuario (Moda)
        # most_common(1) devuelve [(Acci贸n, Cantidad)], usamos [0][0] para sacar la acci贸n.
        most_common = Counter(self.history).most_common(1)[0][0]
        
        # 3. CONTRAATAQUE: Buscar en las reglas qu茅 gana a esa jugada frecuente
        for action, losers in Victories.items():
            if most_common in losers:
                return action
        
        # Fallback de seguridad
        return GameAction(random.randint(0, len(GameAction) - 1))
